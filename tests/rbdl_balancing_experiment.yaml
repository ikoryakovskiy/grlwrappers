experiment: 
  type: experiment/online_learning
  runs: 1
  trials: 0
  steps: 50000
  rate: 0
  test_interval: 10
  output: rbdl_balancing_environment
  environment: 
    type: environment/post/converting
    environment: 
      type: environment/sandbox
      model: 
        type: sandbox_model/leo_walk
        control_step: 0.0003
        integration_steps: 1
        target_dof: 9
        dynamics: 
          type: dynamics/rbdl
          file: leo_vc/leo_ff_dl.lua
          points: tip_left, tip_right, heel_left, heel_right
          auxiliary: ptpos, ptvel, com
        animation: nope
        mode: vc
        knee_mode: punish_and_continue
      task: 
        type: task/leo_balancing
        timeout: 20
        randomize: 5
        measurement_noise: 0
        rwForward: 0
        knee_mode: experiment/environment/environment/model/knee_mode
    converter: 
      type: converter/state_action_converter
      state_in: rlwTorsoX, rlwTorsoZ, rlwTorsoAngle, rlwLeftHipAngle, rlwRightHipAngle, rlwLeftKneeAngle, rlwRightKneeAngle, rlwLeftAnkleAngle, rlwRightAnkleAngle, rlwTorsoXRate, rlwTorsoZRate, rlwTorsoAngleRate, rlwLeftHipAngleRate, rlwRightHipAngleRate, rlwLeftKneeAngleRate, rlwRightKneeAngleRate, rlwLeftAnkleAngleRate, rlwRightAnkleAngleRate, forwardPromotion
      state_out: rlwTorsoAngle, rlwLeftHipAngle, rlwRightHipAngle, rlwLeftKneeAngle, rlwRightKneeAngle, rlwLeftAnkleAngle, rlwRightAnkleAngle, rlwTorsoAngleRate, rlwLeftHipAngleRate, rlwRightHipAngleRate, rlwLeftKneeAngleRate, rlwRightKneeAngleRate, rlwLeftAnkleAngleRate, rlwRightAnkleAngleRate, forwardPromotion
      action_in: rlwLeftHipAction, rlwRightHipAction, rlwLeftKneeAction, rlwRightKneeAction, rlwLeftAnkleAction, rlwRightAnkleAction
      action_out: rlwTorsoXAction, rlwTorsoZAction, rlwTorsoAction, rlwLeftHipAction, rlwRightHipAction, rlwLeftKneeAction, rlwRightKneeAction, rlwLeftAnkleAction, rlwRightAnkleAction
    target_observation_min: experiment/environment/environment/task/observation_min
    target_observation_max: experiment/environment/environment/task/observation_max
    target_action_min: experiment/environment/environment/task/action_min
    target_action_max: experiment/environment/environment/task/action_max
    target_reward_min: experiment/environment/environment/task/reward_min
    target_reward_max: experiment/environment/environment/task/reward_max
  agent: 
    type: agent/fixed
    policy: 
      type: mapping/policy/parameterized/pid
      setpoint: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
      outputs: experiment/environment/environment/task/action_dims
      p: []
      i: []
      d: []
      il: []
      action_min: experiment/environment/environment/task/action_min
      action_max: experiment/environment/environment/task/action_max
  test_agent: experiment/agent
  save_every: run
